{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AISec_Miderm_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z9rbN7LwKi9",
        "colab_type": "text"
      },
      "source": [
        "# Cloud-based PE Malware Detection API:\n",
        "\n",
        "The purpose of this term project is to demonstrate your practical skills in implementing and deploying machine learning models for malware classification. The technical implementation of this project is comprised of three main tasks that need to be completed sequentially:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erYfd8xnwKhh",
        "colab_type": "text"
      },
      "source": [
        "**Task 1 - Training:** In this task, you will be creating and training a deep neural network based on the MalConv architecture to classify PE files as malware or benign. As for the dataset, you will be using the EMBER-2017 v2 ( https://github.com/endgameinc/ember ).\n",
        "\n",
        "If you explore the EMBER repository, you will find that it comes with a sample implementation of MalConv ( https://github.com/endgameinc/ember/tree/master/malconv ). This sample is a wonderful resource to base your implementation on. However, note that this code is 2 years (i.e., a lifetime in ML) old, and does not precisely conform to the requirements of this project.\n",
        "\n",
        "**Implementation:** The model must be implemented in Python 3.x using TensorFlow (1.x or 2.x) and Keras, and needs to be coded and documented in a Jupyter Notebook. Additionaly, add textual description blocks to the notebook to document and explain the different parts of your code.\n",
        "\n",
        "**Training:** This model may take a long time to train on your personal computers (from 7-8 hours to a couple of days, depending on the config), unless you already have a powerful NVIDIA GPU (1080 TI or better). Alternatively, you can use the cloud platforms to speed up the training: Google Colab or AWS Sagemaker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwk52yELeIvD",
        "colab_type": "text"
      },
      "source": [
        "# Deep Neural Network Model on EMBER Malware Dataset:\n",
        "\n",
        "The EMBER dataset is a collection of features from PE files that serve as a benchmark dataset for researchers. <br>\n",
        "In this notebook, the EMBER-2017 v2 dataset is used which contains features from 1.1 million PE files scanned in or before 2017."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eyXmUXtlQma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing required modules\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEqRxmdlWO5V",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Extraction:\n",
        "To use the dataset in this notebook, simple download and upload didn't work as the URL to download the dataset is detected as untrused by Google. So, downloading the EMBER 2017 v2 dataset to Colab notebook using wget command and double unzipping it to get the "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOIiSwMrcLye",
        "colab_type": "code",
        "outputId": "8e74a152-48be-4b9e-d74a-ee90fdad8e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "!wget https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2 --no-check-certificate"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 02:36:25--  https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2\n",
            "Resolving pubdata.endgame.com (pubdata.endgame.com)... 64.250.189.21\n",
            "Connecting to pubdata.endgame.com (pubdata.endgame.com)|64.250.189.21|:443... connected.\n",
            "WARNING: cannot verify pubdata.endgame.com's certificate, issued by ‘CN=Go Daddy Secure Certificate Authority - G2,OU=http://certs.godaddy.com/repository/,O=GoDaddy.com\\\\, Inc.,L=Scottsdale,ST=Arizona,C=US’:\n",
            "  Issued certificate has expired.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1751237573 (1.6G) [application/octet-stream]\n",
            "Saving to: ‘ember_dataset_2017_2.tar.bz2.1’\n",
            "\n",
            "      ember_dataset   0%[                    ]   5.33M  6.38MB/s               ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGf1T6E6eGwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decompressing a .bz2 file\n",
        "!bzip2 -d ember_dataset_2017_2.tar.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rMLNJBJdZRK",
        "colab_type": "code",
        "outputId": "2917b262-178e-4c2e-c6bc-d551b2c31345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "# Extracting from tar file\n",
        "!tar -xvf ember_dataset_2017_2.tar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ember_2017_2/\n",
            "ember_2017_2/train_features_1.jsonl\n",
            "ember_2017_2/train_features_0.jsonl\n",
            "ember_2017_2/train_features_3.jsonl\n",
            "ember_2017_2/test_features.jsonl\n",
            "ember_2017_2/train_features_5.jsonl\n",
            "ember_2017_2/train_features_4.jsonl\n",
            "ember_2017_2/train_features_2.jsonl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tlnm_lzgkCMH",
        "colab_type": "text"
      },
      "source": [
        "All the required dataset files are extracted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGa3ZuR5gYXM",
        "colab_type": "text"
      },
      "source": [
        "Now to work with the EMBER dataset, we need to clone its github repository whihc can be done by following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8aZDlrOhf7F",
        "colab_type": "code",
        "outputId": "383ae0e8-b7c0-4197-fa35-b07490759558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "!git clone https://github.com/endgameinc/ember "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ember'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 215 (delta 9), reused 15 (delta 6), pack-reused 192\u001b[K\n",
            "Receiving objects: 100% (215/215), 11.35 MiB | 13.80 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vrdHctlgYCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv ember ember-master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltF44yxegVzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r ember-master/* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR3VMwDmt6bP",
        "colab_type": "code",
        "outputId": "d3e8fcf3-d384-4ab9-b3f3-643c63f43e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lief>=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/38/e6bf942cf2ee073bf81fa3324bca35409175312b7b72d71919c8fc8e547b/lief-0.10.1-cp36-cp36m-manylinux1_x86_64.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.31.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.38.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.18.3)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.0.3)\n",
            "Requirement already satisfied: lightgbm>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->-r requirements.txt (line 4)) (2018.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm>=2.2.3->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->-r requirements.txt (line 6)) (0.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24.2->-r requirements.txt (line 4)) (1.12.0)\n",
            "Installing collected packages: lief\n",
            "Successfully installed lief-0.10.1\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating ember.egg-info\n",
            "writing ember.egg-info/PKG-INFO\n",
            "writing dependency_links to ember.egg-info/dependency_links.txt\n",
            "writing requirements to ember.egg-info/requires.txt\n",
            "writing top-level names to ember.egg-info/top_level.txt\n",
            "writing manifest file 'ember.egg-info/SOURCES.txt'\n",
            "reading manifest file 'ember.egg-info/SOURCES.txt'\n",
            "writing manifest file 'ember.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/ember\n",
            "copying ember/__init__.py -> build/lib/ember\n",
            "copying ember/features.py -> build/lib/ember\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/ember\n",
            "copying build/lib/ember/__init__.py -> build/bdist.linux-x86_64/egg/ember\n",
            "copying build/lib/ember/features.py -> build/bdist.linux-x86_64/egg/ember\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ember/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/ember/features.py to features.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying ember.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/ember-0.1.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing ember-0.1.0-py3.6.egg\n",
            "Copying ember-0.1.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding ember 0.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/ember-0.1.0-py3.6.egg\n",
            "Processing dependencies for ember==0.1.0\n",
            "Searching for scikit-learn==0.22.2.post1\n",
            "Best match: scikit-learn 0.22.2.post1\n",
            "Adding scikit-learn 0.22.2.post1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for lightgbm==2.2.3\n",
            "Best match: lightgbm 2.2.3\n",
            "Adding lightgbm 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pandas==1.0.3\n",
            "Best match: pandas 1.0.3\n",
            "Adding pandas 1.0.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.3\n",
            "Best match: numpy 1.18.3\n",
            "Adding numpy 1.18.3 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tqdm==4.38.0\n",
            "Best match: tqdm 4.38.0\n",
            "Adding tqdm 4.38.0 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for lief==0.10.1\n",
            "Best match: lief 0.10.1\n",
            "Adding lief 0.10.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for joblib==0.14.1\n",
            "Best match: joblib 0.14.1\n",
            "Adding joblib 0.14.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.12.0\n",
            "Best match: six 1.12.0\n",
            "Adding six 1.12.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for ember==0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Cr_zbZWEw_",
        "colab_type": "text"
      },
      "source": [
        "The LIEF project is used to extract features from PE files included in the EMBER dataset. Raw features are extracted to JSON format. Vectorized features can be produced from these raw features and saved in binary format from which they can be converted to CSV, dataframe, or any other format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJgzku0t7hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ember\n",
        "ember.create_vectorized_features(\"/content/ember_2017_2/\")\n",
        "ember.create_metadata(\"/content/ember_2017_2/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6J-vmo77zYH",
        "colab_type": "code",
        "outputId": "79668f15-26a5-41e7-ca03-54cf06b136a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "import ember\n",
        "data_path = '/content/ember_2017_2/'\n",
        "emberdf = ember.read_metadata(data_path)\n",
        "emberdf.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sha256</th>\n",
              "      <th>appeared</th>\n",
              "      <th>subset</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...</td>\n",
              "      <td>2006-12</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d4206650743b3d519106dea10a38a55c30467c3d9f7875...</td>\n",
              "      <td>2006-12</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...</td>\n",
              "      <td>2007-01</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7f513818bcc276c531af2e641c597744da807e21cc1160...</td>\n",
              "      <td>2007-02</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...</td>\n",
              "      <td>2007-02</td>\n",
              "      <td>train</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sha256 appeared subset  label\n",
              "0  0abb4fda7d5b13801d63bee53e5e256be43e141faa077a...  2006-12  train      0\n",
              "1  d4206650743b3d519106dea10a38a55c30467c3d9f7875...  2006-12  train      0\n",
              "2  c9cafff8a596ba8a80bafb4ba8ae6f2ef3329d95b85f15...  2007-01  train      0\n",
              "3  7f513818bcc276c531af2e641c597744da807e21cc1160...  2007-02  train      0\n",
              "4  ca65e1c387a4cc9e7d8a8ce12bf1bcf9f534c9032b9d95...  2007-02  train      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhATFqoa81qR",
        "colab_type": "code",
        "outputId": "364be1a3-0324-4a8f-f955-3142380a871d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X_train0, y_train0, X_test0, y_test0 = ember.read_vectorized_features(data_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.10.1-bfe5414 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX3zlnp6bBH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZhOHnFA88Z-",
        "colab_type": "code",
        "outputId": "81be2f06-b4f8-442c-a06a-66966a548bf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#shape of the dataset\n",
        "X_train0.shape, y_train0.shape, X_test0.shape, y_test0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((900000, 2381), (900000,), (200000, 2381), (200000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XexOgKaqkfZ_",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjmcemBkkd67",
        "colab_type": "text"
      },
      "source": [
        "It is known that the EMBER train dataset has three sample categories, namels unlabled, benign and malicious. They are represented as -1, 0 and 1 respectively. But it can be seen that the test dataset has only benign and malicious samples. In this project, I am ignoring the unlabled samples from the train dataset for the better performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrX0zh4gOoqT",
        "colab_type": "code",
        "outputId": "02036158-34ab-408a-d15b-4e5f45770c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import pandas as pd\n",
        "# Creating dataframes of X_train & y_train\n",
        "X_train0 = pd.DataFrame(X_train0)\n",
        "y_train0 = pd.DataFrame(y_train0)\n",
        "X_train0.shape, y_train0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600000, 2381), (600000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrUqD29-Ukuz",
        "colab_type": "code",
        "outputId": "4616370d-fa0f-4654-ea36-e1830d1e297f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#Unique labels in the train dataset \n",
        "y_train0[0].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9DJTT2wZDU-",
        "colab_type": "code",
        "outputId": "b0b0e354-d285-4a3e-d9d2-af6ea6d5297b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Combining features and lables of train dataset\n",
        "X_train0[2381] = y_train0[0]\n",
        "X_train0.shape, y_train0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600000, 2382), (600000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3s_F9dYZqpZ",
        "colab_type": "code",
        "outputId": "c972fec5-f17e-445b-fbb1-5921b845a3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#Checking the presence of unique lables in the combined dataframe\n",
        "X_train0[2381].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNlo43Q8S-af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removing the unlabeled rows from the dataframe\n",
        "\n",
        "X_train0.drop(X_train0[(X_train0[2381] == -1)].index, inplace = True)\n",
        "y_train0.drop(y_train0[(y_train0[0] == -1)].index, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFqNvoNxiGQX",
        "colab_type": "code",
        "outputId": "3c67f7a8-2d63-41ec-ab59-718b1380cd5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X_train0.shape, y_train0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600000, 2382), (600000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s9OqjnseC7_",
        "colab_type": "code",
        "outputId": "75ee627f-3df1-468b-888d-e134d704dd47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#reconstructing the X_train dataframe\n",
        "X_train0.drop([2381], axis =1, inplace=True)\n",
        "X_train0.shape, y_train0.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600000, 2381), (600000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JZZs4MwXXgi",
        "colab_type": "text"
      },
      "source": [
        "The dataset is huge and takes lot to time for vectorizing and creating metadata for every runtime execution. So, create pickle files for the training and testing samples to store them in the system. By downloading and storing these pickle files, one can avoid the execution of the former lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xGtAaozdr9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Pickling the datasets\n",
        "pd.DataFrame(X_train0).to_pickle(\"./X_train.pkl\")\n",
        "pd.DataFrame(y_train0).to_pickle(\"./y_train.pkl\")\n",
        "pd.DataFrame(X_test0).to_pickle(\"./X_test.pkl\")\n",
        "pd.DataFrame(y_test0).to_pickle(\"./y_test.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkPA_DWw5T3k",
        "colab_type": "text"
      },
      "source": [
        "I faced network fialure error while downloading the pickle file to store it in my system. The alternate solution for this error is to upload the pickle files to the Google Drive by executing the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGz-025xs8uW",
        "colab_type": "code",
        "outputId": "fae012d9-bed9-4246-94f3-a7db7e9c51e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "# To upload the files, mount the Google drive in the colab. Go the URL obtained after executing the below lines of code \n",
        "#and do the necessary allows for the Google access. Then at the end an authorization code is displayed.\n",
        "#Copy the code and paste it in the box below \"Enter your authorization code:\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFleRQU8uG8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copying pickle files to Google Drive\n",
        "!cp ./X_test.pkl ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./y_test.pkl ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./X_train.pkl ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./y_train.pkl ./gdrive/My\\ Drive/Pickle_Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnczpAhUzMD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting training data from pickle files\n",
        "X_trainp = pd.read_pickle(\"/content/gdrive/My Drive/Pickle_Files/X_train.pkl\")\n",
        "y_trainp = pd.read_pickle(\"/content/gdrive/My Drive/Pickle_Files/y_train.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb7CB5cBQwj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extracting testing data from pickle files\n",
        "X_testp =pd.read_pickle(\"/content/gdrive/My Drive/Pickle_Files/X_test.pkl\")\n",
        "y_testp = pd.read_pickle(\"/content/gdrive/My Drive/Pickle_Files/y_test.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1UyM8a00pmM",
        "colab_type": "code",
        "outputId": "2e6d8c26-bcdc-416d-d1bb-a3f478484dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Shape of the dataset\n",
        "X_trainp.shape, y_trainp.shape, X_testp.shape, y_testp.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((900000, 2381), (900000, 1), (200000, 2381), (200000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK89OZPAZ3YJ",
        "colab_type": "text"
      },
      "source": [
        "At this point of execution, I can see that the above lines of code used most of the 25GB RAM availbale in Colab. So, even though the datasets are pickled, the RAM crashes. The alternative for this is to create HDF5 files. The h5py package is a Pythonic interface to the HDF5 binary data format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZRZoyXPZ2Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "\n",
        "# Loading X_train data to HDF5 file\n",
        "h50 = h5py.File('X_train0.h5', 'w')\n",
        "h50.create_dataset('X_train0', data=X_train0)\n",
        "h50.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdmoZhq1E_Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading y_train data to HDF5 file\n",
        "h51 = h5py.File('y_train0.h5', 'w')\n",
        "h51.create_dataset('y_train0', data=y_train0)\n",
        "h51.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5vRvd4FX5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading X_test data to HDF5 file\n",
        "h52 = h5py.File('X_test0.h5', 'w')\n",
        "h52.create_dataset('X_test0', data=X_test0)\n",
        "h52.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NSN1YzZFKsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading y_test data to HDF5 file\n",
        "h53 = h5py.File('y_test0.h5', 'w')\n",
        "h53.create_dataset('y_test0', data=y_test0)\n",
        "h53.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_OnOivjSg_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Storing all the h5 files to GDrive\n",
        "!cp ./X_train0.h5 ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./y_train0.h5 ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./X_test0.h5 ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./y_test0.h5 ./gdrive/My\\ Drive/Pickle_Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNvZBLVLHV-",
        "colab_type": "code",
        "outputId": "99ee786f-c148-475c-dedb-302c7f4b91ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#reading the X_train data from h5 files \n",
        "import h5py\n",
        "Xh5 = h5py.File('/content/gdrive/My Drive/Pickle_Files/X_train0.h5','r')\n",
        "X_train = Xh5['X_train0']\n",
        "X_train.shape "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600000, 2381)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvCiIwcRLmRK",
        "colab_type": "code",
        "outputId": "25db2f88-41ff-4099-f56f-2e57d796c4cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Reading y_train data from h5 files\n",
        "import h5py\n",
        "yh5 = h5py.File('/content/gdrive/My Drive/Pickle_Files/y_train0.h5','r')\n",
        "y_train = yh5['y_train0']\n",
        "y_train.shape "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_9RdRv0asUh",
        "colab_type": "code",
        "outputId": "c9c5c635-07d8-42a1-9680-bb4a834c9a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Reading X_test data from h5 files\n",
        "import h5py\n",
        "Xth5 = h5py.File('/content/gdrive/My Drive/Pickle_Files/X_test0.h5','r')\n",
        "X_test = Xth5['X_test0']\n",
        "X_test.shape "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 2381)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DvD6F6QasIN",
        "colab_type": "code",
        "outputId": "b95d76be-ed23-44c5-96b0-bb8785aade14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Reading y_test data from h5 files\n",
        "import h5py\n",
        "yth5 = h5py.File('/content/gdrive/My Drive/Pickle_Files/y_test0.h5','r')\n",
        "y_test = yth5['y_test0']\n",
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7iepfIOI3FC",
        "colab_type": "text"
      },
      "source": [
        "**The features of this dataset are scaled on different scalars and among them I picked RobustScalar to do the feature scaling.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2MVL53fHFXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scaling the features inorder to improve the performance of the model\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "rs = RobustScaler()\n",
        "Xtrain_rs = rs.fit_transform(X_train)\n",
        "Xtest_rs = rs.fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrHijvhMpcBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading scaled X_train data to HDF5 file\n",
        "h54 = h5py.File('Xtrain_rs.h5', 'w')\n",
        "h54.create_dataset('Xtrain_rs', data=Xtrain_rs)\n",
        "h54.close()\n",
        "\n",
        "#Storing the h5 files to GDrive\n",
        "!cp ./Xtrain_rs.h5 ./gdrive/My\\ Drive/Pickle_Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Fdt27vpADT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading scaled X_test data to HDF5 file\n",
        "h55 = h5py.File('Xtest_rs.h5', 'w')\n",
        "h55.create_dataset('Xtest_rs', data=Xtest_rs)\n",
        "h55.close()\n",
        "\n",
        "#Storing the h5 files to GDrive\n",
        "!cp ./Xtest_rs.h5 ./gdrive/My\\ Drive/Pickle_Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsA_AgUwqyf9",
        "colab_type": "code",
        "outputId": "7e4509a6-9443-4e60-c53c-75dd76868e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Reading Xtrain_rs data from h5 files\n",
        "import h5py\n",
        "Xrsh5 = h5py.File('/content/gdrive/My Drive/Pickle_Files/Xtrain_rs.h5','r')\n",
        "Xtrain_rs = Xrsh5['Xtrain_rs']\n",
        "Xtrain_rs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(600000, 2381)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3yzcRH4qyVg",
        "colab_type": "code",
        "outputId": "99535da5-d90b-4090-8f2c-77848ba7542a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Reading Xtest_rs data from h5 files\n",
        "import h5py\n",
        "Xtrsh5 = h5py.File('/content/gdrive/My Drive/Pickle_Files/Xtest_rs.h5','r')\n",
        "Xtest_rs = Xtrsh5['Xtest_rs']\n",
        "Xtest_rs.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 2381)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPNEf-Kd3tsW",
        "colab_type": "text"
      },
      "source": [
        "## Model Arcitecture & Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZKXIPIJ3S3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for the model\n",
        "def myModel():\n",
        "\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from keras import regularizers\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    \n",
        "    #Model architecture\n",
        "    model = Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=(2381,))) \n",
        "    model.add(layers.Dropout(0.2))\n",
        "    model.add(layers.Dense(units = 1000, activation = tf.nn.relu, activity_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(units = 1, activation=tf.nn.sigmoid))\n",
        "    print(model.summary())\n",
        "    \n",
        "    #model compilation\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.save('my_model.h5')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAGbdm0saCKL",
        "colab_type": "code",
        "outputId": "6d552ec5-f9cc-41df-82b4-4ed20e36e0e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "model = myModel()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dropout (Dropout)            (None, 2381)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1000)              2382000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 1001      \n",
            "=================================================================\n",
            "Total params: 2,383,001\n",
            "Trainable params: 2,383,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6aNmXj_5f8q",
        "colab_type": "code",
        "outputId": "507bde96-ac64-4e23-a873-1fcbd4a1f13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#Training the model on 1 epoch\n",
        "history = model.fit(Xtrain_rs, y_train,\n",
        "                batch_size=256, shuffle=\"batch\",\n",
        "                epochs=1, \n",
        "                validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480000 samples, validate on 120000 samples\n",
            "Epoch 1/1\n",
            "480000/480000 [==============================] - 75s 157us/step - loss: 199371956481915.2188 - accuracy: 0.5161 - val_loss: 1433708113807.5051 - val_accuracy: 0.3903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0bX1X32xLri",
        "colab_type": "code",
        "outputId": "38669a21-ae8a-4fa2-ceed-343074e13a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(Xtrain_rs, y_train,\n",
        "                batch_size=256, shuffle=\"batch\",\n",
        "                epochs=30, \n",
        "                validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 480000 samples, validate on 120000 samples\n",
            "Epoch 1/30\n",
            "480000/480000 [==============================] - 75s 156us/step - loss: 6178487812691.4307 - accuracy: 0.5247 - val_loss: 1969192253130.7263 - val_accuracy: 0.4556\n",
            "Epoch 2/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 1058981049957.0898 - accuracy: 0.5552 - val_loss: 3813439811382.9297 - val_accuracy: 0.4401\n",
            "Epoch 3/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 229784464899.2173 - accuracy: 0.5377 - val_loss: 2560687258697.7104 - val_accuracy: 0.4306\n",
            "Epoch 4/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 1016042836667.2339 - accuracy: 0.5256 - val_loss: 574407725213.3739 - val_accuracy: 0.4072\n",
            "Epoch 5/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 105944078490.9681 - accuracy: 0.5293 - val_loss: 701150794838.2307 - val_accuracy: 0.4175\n",
            "Epoch 6/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 39826254685.3327 - accuracy: 0.5111 - val_loss: 256010935122.9343 - val_accuracy: 0.4479\n",
            "Epoch 7/30\n",
            "480000/480000 [==============================] - 77s 161us/step - loss: 49011224045.6871 - accuracy: 0.5085 - val_loss: 3021691767575.7310 - val_accuracy: 0.4261\n",
            "Epoch 8/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 104332267364.9166 - accuracy: 0.5078 - val_loss: 545956168777.0072 - val_accuracy: 0.4284\n",
            "Epoch 9/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 50759298974.8968 - accuracy: 0.5075 - val_loss: 804209375067.8447 - val_accuracy: 0.4227\n",
            "Epoch 10/30\n",
            "480000/480000 [==============================] - 80s 166us/step - loss: 339979163342.6208 - accuracy: 0.5052 - val_loss: 400114026152.0847 - val_accuracy: 0.4240\n",
            "Epoch 11/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 342160312177.5456 - accuracy: 0.5059 - val_loss: 458486425814.5881 - val_accuracy: 0.4387\n",
            "Epoch 12/30\n",
            "480000/480000 [==============================] - 75s 157us/step - loss: 701877176294.9227 - accuracy: 0.5055 - val_loss: 135395854613.2970 - val_accuracy: 0.4427\n",
            "Epoch 13/30\n",
            "480000/480000 [==============================] - 77s 160us/step - loss: 1870140593754.5503 - accuracy: 0.5088 - val_loss: 143314214819.6264 - val_accuracy: 0.4518\n",
            "Epoch 14/30\n",
            "480000/480000 [==============================] - 76s 157us/step - loss: 261664778228.1376 - accuracy: 0.5122 - val_loss: 1053137208764.1531 - val_accuracy: 0.4460\n",
            "Epoch 15/30\n",
            "480000/480000 [==============================] - 76s 159us/step - loss: 59661257698.9393 - accuracy: 0.5099 - val_loss: 6802059700625.0752 - val_accuracy: 0.4433\n",
            "Epoch 16/30\n",
            "480000/480000 [==============================] - 77s 160us/step - loss: 646433572171.8712 - accuracy: 0.5105 - val_loss: 4541052554838.7188 - val_accuracy: 0.4452\n",
            "Epoch 17/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 189729421627.3615 - accuracy: 0.5154 - val_loss: 1242129291808.7312 - val_accuracy: 0.4493\n",
            "Epoch 18/30\n",
            "480000/480000 [==============================] - 75s 157us/step - loss: 1364475557234.0129 - accuracy: 0.5171 - val_loss: 1989266234658.7402 - val_accuracy: 0.4478\n",
            "Epoch 19/30\n",
            "480000/480000 [==============================] - 77s 160us/step - loss: 1415480678417.3452 - accuracy: 0.5155 - val_loss: 449924351412.7711 - val_accuracy: 0.4534\n",
            "Epoch 20/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 197156019379.8801 - accuracy: 0.5178 - val_loss: 1683445315006.3015 - val_accuracy: 0.4563\n",
            "Epoch 21/30\n",
            "480000/480000 [==============================] - 74s 154us/step - loss: 37020623782.0104 - accuracy: 0.5202 - val_loss: 1846134533084.9065 - val_accuracy: 0.4560\n",
            "Epoch 22/30\n",
            "480000/480000 [==============================] - 74s 154us/step - loss: 202639031855.1043 - accuracy: 0.5175 - val_loss: 964622513682.7982 - val_accuracy: 0.4506\n",
            "Epoch 23/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 16942802998.9931 - accuracy: 0.5168 - val_loss: 486390543683.4103 - val_accuracy: 0.4592\n",
            "Epoch 24/30\n",
            "480000/480000 [==============================] - 74s 154us/step - loss: 1782897617438.5427 - accuracy: 0.5177 - val_loss: 1057376023146.0570 - val_accuracy: 0.4548\n",
            "Epoch 25/30\n",
            "480000/480000 [==============================] - 74s 155us/step - loss: 6324107412755.8467 - accuracy: 0.5195 - val_loss: 908728256234.3539 - val_accuracy: 0.4615\n",
            "Epoch 26/30\n",
            "480000/480000 [==============================] - 75s 156us/step - loss: 135793908627.4476 - accuracy: 0.5193 - val_loss: 2061703311421.6003 - val_accuracy: 0.4576\n",
            "Epoch 27/30\n",
            "480000/480000 [==============================] - 74s 155us/step - loss: 72855577711.3204 - accuracy: 0.5158 - val_loss: 924201204232.3602 - val_accuracy: 0.4567\n",
            "Epoch 28/30\n",
            "480000/480000 [==============================] - 74s 155us/step - loss: 757651698265.1063 - accuracy: 0.5158 - val_loss: 1068298676362.8690 - val_accuracy: 0.4551\n",
            "Epoch 29/30\n",
            "480000/480000 [==============================] - 76s 158us/step - loss: 67442393709.6003 - accuracy: 0.5154 - val_loss: 2266757227566.1035 - val_accuracy: 0.4539\n",
            "Epoch 30/30\n",
            "480000/480000 [==============================] - 74s 155us/step - loss: 485824225874.6187 - accuracy: 0.5156 - val_loss: 1446379017887.5652 - val_accuracy: 0.4612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg4sROzAX6lf",
        "colab_type": "text"
      },
      "source": [
        "## Model Testing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyQp8WrpYBzY",
        "colab_type": "code",
        "outputId": "5c9fb1a1-ad3f-459f-9fef-7f2e71da74b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# testing the model\n",
        "\n",
        "score =model.evaluate(Xtest_rs,y_test)\n",
        "print(\"Training accuracy:\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200000/200000 [==============================] - 35s 175us/step\n",
            "Training accuracy: 0.4422149956226349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_J-0j2NauR",
        "colab_type": "text"
      },
      "source": [
        "Now, lets save the model for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBqnfO5DNYXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "#model.save('my_model.h5')\n",
        "model.save_weights('my_model_weights.h5')\n",
        "\n",
        "#Storing the model to GDrive\n",
        "!cp ./my_model.h5 ./gdrive/My\\ Drive/Pickle_Files\n",
        "!cp ./my_model_weights.h5 ./gdrive/My\\ Drive/Pickle_Files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VWp3qzSOEZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save neural network structure to JSON (no weights)\n",
        "model_json = model.to_json()\n",
        "with open(\"mymodeljson.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights(\"my_model-weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RuDAKif17za",
        "colab_type": "text"
      },
      "source": [
        "The below set of code is a a function that takes a PE file as its argument, runs it through the trained model, and returns the output i.e., 1 for Malware or ) for Benign."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9ZQ-eXWxiby",
        "colab_type": "code",
        "outputId": "96d91f8c-594c-4bae-f1ae-cdaa5852fd2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "!wget https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 03:17:13--  https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 488908696 (466M) [application/octet-stream]\n",
            "Saving to: ‘Anaconda3-2020.02-Windows-x86_64.exe’\n",
            "\n",
            "Anaconda3-2020.02-W 100%[===================>] 466.26M   201MB/s    in 2.3s    \n",
            "\n",
            "2020-04-28 03:17:15 (201 MB/s) - ‘Anaconda3-2020.02-Windows-x86_64.exe’ saved [488908696/488908696]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS_N5Gdc5BkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def testPE(pe):\n",
        "  import ember\n",
        "  import numpy as np\n",
        "  import tensorflow as tf\n",
        "  from sklearn.preprocessing import RobustScaler\n",
        "  rs = RobustScaler()\n",
        "  \n",
        "  #opening the downloaded PE file\n",
        "  testpe = open(pe, \"rb\").read()\n",
        "  #Feature extractor class of the ember project \n",
        "  extract = ember.PEFeatureExtractor() \n",
        "  data = extract.feature_vector(testpe) #vectorizing the extracted features\n",
        "  scaled_data = rs.fit_transform([data])\n",
        "  Xdata = np.reshape(scaled_data,(1, 2381))\n",
        "\n",
        "  model = tf.keras.models.load_model('my_model.h5')\n",
        "  pred = model.predict_classes(Xdata)\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBX-cJolozfG",
        "colab_type": "code",
        "outputId": "f5a6a19e-1921-4467-82d5-7cec04ec3074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "testPE(\"Anaconda3-2020.02-Windows-x86_64.exe\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.10.1-bfe5414 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAQVnkXDZC36",
        "colab_type": "text"
      },
      "source": [
        "The model predicted that Anaconda PE file as Benign"
      ]
    }
  ]
}